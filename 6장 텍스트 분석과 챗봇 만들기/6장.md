## 6장 텍스트 분석과 챗봇 만들기

`목표` 머신러닝을 이용한 텍스트 분석을 살펴보고 간단한 챗봇 만들어보기



### 6-1 한국어 분석(형태소 분석)

> ### 형태소 분석

* 자연 언어의 문장을 형태소라는 **의미를 갖는 최소 단위로 분할**하고, **품사를 판별**하는 작업
* 기계 번역, 텍스트 마이닝 등의 여러 분야에서 활용되고 있다.
* 방법은 크게 2가지로 **문법 규칙에 의한 방법**과 **확률적 언어 모델을 사용하는 방법**이 있는데, 최근에는 확률적 언어 모델을 사용한 형태소 분석이 많아져서 정밀도가 높아졌다.
* 두 가지 모두 품사 사전과 문법 사전을 기반으로 대조하면서 형태소를 분석한다.



> ### 한국어 형태소 분석 라이브러리

* 라이브러리를 이용하면 직접 사전을 준비하고, 형태소 분석 알고리즘을 구현하지 않아도 형태소 분석을 할 수 있다.
* 이 책에서는 KoNLPy 라이브러리를 이용하여 트위터 형태소 분석기를 사용해볼 것이다.





### 6-2 Word2Vec으로 문장을 벡터로 변환하기

> ### Word2Vec

* 문장 내부의 단어를 **벡터로 변환**하는 도구
* 단어의 연결을 기반으로 **단어의 연관성**을 벡터로 만들어준다.
* 이를 활용하면 단어의 의미를 파악할 수 있다.
* 벡터를 크게 만들 때는 그만큼 입력하는 텍스트 파일도 커야 하기 때문에 벡터를 무조건 크게 설정한다고 좋은 것이 아니다.
* 특정 단어의 유의어, 반의어를 추출할 수 있으며 단어를 **선형**으로 나타낼 수 있다.
* 대량의 데이터를 Word2Vec에 활용하면 자연 언어 처리에 활용할 수 있다.
* Word2Vec를 구현하는 도구는 굉장히 많은데 이 책에서는 **Gensim 라이브러리**(자연 언어 처리를 위한 라이브러리)를 이용한다.





### 6-3 베이즈 정리로 텍스트 분류하기

> ### 텍스트 분류

* 자주 사용되는 방법으로는 베이즈 정리를 이용한 텍스트 분류 방법인 **베이지안 필터**(Bayesian filter)가 있다.
  * 학습을 많이 시키면 시킬수록 필터의 분류 능력이 오른다는 특징이 있다.
  * 베이지안 필터는 **교사 학습**에 해당된다.
  * 메일 서비스에서 스팸 메일을 구분하거나 커뮤니티 사이트에서 스팸 글을 구분할 때 많이 사용된다.



> ### 베이즈 정리

* 조건부 확률과 관련된 이론으로, 토머스 베이즈에 의해 정립된 이론
* P(B|A) = P(A|B)*P(B)/P(A)



> ### 조건부 확률

* P(B|A) : 어떤 A라는 사건이 일어났다는 조건에서 다른 사건 B가 일어날 확률



> ### 나이브 베이지 분류

* 베이즈 정리를 사용한 분류 방법
* 베이지안 필터가 나이브 베이지 분류라는 알고리즘을 사용한다.
* 텍스트 내부에서 단어 출현 비율을 조사하고 이를 기반으로 해당 텍스트를 어떤 카테고리로 분류하는 것이 적합한지 알아본다.
* P(B|A) = P(B)*P(A|B)





### 6-4 MLP로 텍스트 분류하기

> ### **MLP(Multi Layer Perception)**

* 다층 퍼셉트론으로 입력층과 출력층 사이에 각각 전체 결합하는 은닉층을 넣은 뉴럴 네트워크이다.



> ### **텍스트 데이터를 고정 길이의 벡터로 변환하는 방법**

* 가장 기본적인 방법 : 단어 하나하나에 **ID**를 부여하고, 그러한 ID의 **출현 빈도와 정렬 순서**를 기반으로 벡터를 만드는 방법
* Bow(Bag-of-words) : 글에 어떠한 단어가 있는지를 수치로 나타내는 방법



> ### 텍스트 분류 과정

* 텍스트에서 불필요한 품사를 제거한다.
* 사전을 기반으로 단어를 숫자로 변환한다.
* 파일 내부의 단어 출현 비율을 계산한다.
* 데이터를 학습시킨다.
* 테스트 데이터를 넣어 성공률을 확인한다.



> 텍스트 데이터를 대상으로 머신러닝을 수행할 때는 **텍스트를 숫자 벡터로 변환**해야 하며, MLP가 아닌 다른 여러 가지 방법을 활용해서도 텍스트를 분류할 수 있다.





### 6-5 문장의 유사도를 N-gram으로 분석하기

> ### 문장의 유사도 확인하는 방법 2가지

* **레벨슈타인 거리(Lvenshtein distance)**
  * **두 개의 문자열이 어느 정도 다른지**를 나타내는 것
  * 편집 거리(Edit Distance)라고도 부른다.
  * 철자 오류 수정, 비슷한 어구 검색 등에 사용되고 있는데, 의학 분야에서는 DNA 배열의 유사성을 판단할 때도 사용하고 있다.
* **N-gram**
  * 텍스트에서 이웃한 N개의 문자
  * 서로 다른 2개의 문장을 N-gram으로 비교해보면 출현하는 **단어의 종류와 빈도를 확인**할 수 있다.
  * 이를 활용하면 논문 도용 등을 확인할 수 있다.
  * 또한 프로그램 코드에 적용해 라이선스를 가지고 있는 코드를 복사해서 붙여넣지 않았는지 등도 확인할 수 있다.
  * 쉽게 문장의 **유사도를 확인**할 수 있다.





###  6-6 마르코프 체인과 LSTM으로 문장 생성하기

> ### 문장 생성 방법

* **마르코프 체인**
  * **확률**을 기반으로 하는 방법
  * 러시아의 수학자 마르코프가 연구해서 이러한 이름이 붙었고 워드 샐러드라고도 한다.
  * 물리학과 통계학의 기본 모델로 응용되고 있다.
  * 기존 문장을 기반으로 문장을 자동으로 생성할 수 있다.
  * 문장을 요약하는 기능도 있다.
  * 기계적으로 문장을 생성하고, 트위터에 자동으로 등록하는 트위터봇에도 사용된다.
  * **단어의 실질적인 의미 연관성을 생각하지 않고 문장을 조합해서 조금 이상한 문장이 만들어지는 경우가 많다.**
  * 무작위로 어구를 붙여 문장을 생성한다.
  * 마르코프 성질
    * 과거의 상태를 무시하고, **현재의 상태만**을 기반으로 다음 상태를 선택하는 것을 의미한다.
    * 마르코프 성질이 존재하면 현재 상태를 qi라고 표현했을 때 다음 상태 qj로 이동할 확률은 현재 상태와 다음 상태만을 기준으로 결정되므로 P(qj | qi)이다.
  * 문장을 만드는 과정
    * 문장을 **단어**로 분할(형태소 분석)한다. (N-gram은 **문자** 단위)
    * 단어의 전후 연결을 딕셔너리에 등록한다.
    * 사전을 사용해 임의의 문장을 생성한다.
* LSTM/RNN 
  * RNN(Recurrent Neural Network)
    * 신경망을 재귀적으로 사용해 시간 순서를 가진 데이터를 다룰 수 있게 한 것
    * **바로 전의 데이터밖에 기억하지 못한다.**
  * LSTM(Long Short Term-Memory)
    * RNN을 개량한 것
    * **장기적으로 기억**할 수 있게 여러 가지 기능을 추가한 것
    * 머신러닝으로 어구에 이어서 올 어구를 찾는 방법이다.





### 6-7 챗봇 만들기

> ### 챗봇(회화 봇)

* 사용자의 질문에 **응답**하는 프로그램을 의미한다.
* 상향식 인공지능 접근법으로는 인간다움에 도달하기 힘들므로 하향식 접근법으로 인간다운 모델을 만들어서 인간다움을 만들어 내는 것
* 목적은 사용자와 **대화**를 나누는 것
* 챗봇이 지능을 가지고 대화하는 것처럼 보이지만, 대부분의 챗봇은 키워드를 기반으로 데이터베이스에서 적절한 대답을 찾아 응답할 뿐이다.
* 초기 챗봇은 ELIZA(일라이자) - 환자 중심 상담 이론을 기반으로 1960년대에 만들어진 프로그램
* 구조
  * 사용자의 발언을 전달하면 챗봇은 대화 문장 속에 있는 키워드를 기반으로 데이터베이스에서 정보를 찾아 조합해서 만든 문장을 사용자에게 응답한다.

